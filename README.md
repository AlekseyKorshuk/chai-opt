# Integrate OPT-30B to Chai

# Plan

- [ ] Prepare model
- [ ] Make a draft of an inference in Colab
- [ ] Reduce needed RAM !!! (may be offload state dict)
- [ ] Optimize inference with `float16`
- [ ] Test everything with multiple small checkpoints
- [ ] Somehow understand how to convert it to ONNX without 100500 GB of RAM
- [ ] Create some beautiful graphs with benchmarks
- [ ] Prepare Dockerfile
- [ ] Test locally
- [ ] With a help of God upload to CoreWeave
