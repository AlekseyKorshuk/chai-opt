# Integrate OPT-30B to Chai

# Plan

- [x] Prepare model
- [x] Make a draft of an inference in Colab
- [x] Reduce needed RAM !!! (may be offload state dict)
- [x] Optimize inference with `floatN`
- [ ] Test everything with multiple small checkpoints
- [ ] Somehow understand how to convert it to ONNX without 100500 GB of RAM
- [ ] Create some beautiful graphs with benchmarks
- [ ] Prepare Dockerfile
- [ ] Test locally
- [ ] With a help of God upload to CoreWeave
